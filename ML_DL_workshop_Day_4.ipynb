{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1e7a04ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7a04ae",
        "outputId": "9ad2dc01-c883-4eb4-f253-0baddfdd46da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/suyog/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/suyog/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/suyog/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3674f01",
      "metadata": {
        "id": "f3674f01"
      },
      "source": [
        "## Table of Contents\n",
        "- **What is NLP and how it correlates with aspects of AI?**\n",
        "- **Corpus**\n",
        "- **Vocabulary**\n",
        "- **Tokens and Tokenization**\n",
        "- **Stemming and Lemmatization**\n",
        "- **Vectorization/Embedding**\n",
        "- **Model Training for text-classification**\n",
        "- **Bonus: for the people who support till the end :D**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01731686",
      "metadata": {
        "id": "01731686"
      },
      "source": [
        "# What is Corpus?\n",
        "<br/>\n",
        "\n",
        "**Fundamental Unit of NLP :** `TEXT` \n",
        "\n",
        "**Simply, a collection of text is a** `Corpus`**. We analyse and get insights from the corpus. Hence, Corpus is a dataset for building models** \n",
        "\n",
        "**Examples:**\n",
        "- NewsPaper articles\n",
        "- Essay Book\n",
        "- Reviews on Daraz\n",
        "- Information in Invoices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24be08c8",
      "metadata": {
        "id": "24be08c8"
      },
      "source": [
        "# But, how come Computers understand the TEXT?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5168ae6",
      "metadata": {
        "id": "f5168ae6"
      },
      "source": [
        "# But, how come Computers understand the TEXT?\n",
        "\n",
        "- **There's where the `Vectorization` comes to Play :D**\n",
        "- **But, before vectorizing the text, how many words can a Computer remember?**\n",
        "\n",
        "**Quick Note:** ***We will soon discuss more about the vectorization***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2fe0e5",
      "metadata": {
        "id": "fd2fe0e5"
      },
      "source": [
        "## Vocabulary\n",
        "\n",
        "- **Analogous to dictionary we have**\n",
        "- **A set of words**(***token***) **are chosen based on the number of time it appears in the Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54e4d24e",
      "metadata": {
        "id": "54e4d24e"
      },
      "source": [
        "## Tokens and Tokenization\n",
        "\n",
        "- **Tokens are basically the smaller units that can be more easily assigned meaning**\n",
        "\n",
        "**Example: For the sentence `Today we are learning NLP`, the tokens are `[Today, we, are, learning, NLP]`**\n",
        "\n",
        "**In dictionary,**\n",
        "\n",
        "```python \n",
        "{\n",
        "    \"Today\": 0, \n",
        "    \"We\": 1, \n",
        "    \"are\": 2,\n",
        "    \"learning\": 3,\n",
        "    \"NLP\": 4\n",
        "}\n",
        "```\n",
        "\n",
        "**The above process of splitting the corpus to individual words and assigning a numeric value to each of them is known as tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0b7780de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b7780de",
        "outputId": "7fc44119-bdb9-401d-cf8a-83a6da83e2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tokens for above text are: ['Today', 'we', 'are', 'learning', 'NLP']\n",
            "In dictionary, \n",
            "{0: 'Today', 1: 'we', 2: 'are', 3: 'learning', 4: 'NLP'}\n"
          ]
        }
      ],
      "source": [
        "text = \"Today we are learning NLP\"\n",
        "tokens = text.split()    # Creating tokens using .split() method\n",
        "token2id = {i:j for i,j in enumerate(tokens)}    # Creating dictionary\n",
        "\n",
        "print(f\"The tokens for above text are: {tokens}\")\n",
        "print(\"In dictionary, \", token2id, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0987f77c",
      "metadata": {
        "id": "0987f77c"
      },
      "source": [
        "## Problem with .split() method for Tokenization\n",
        "\n",
        "- **Doesn't consider punctuation while creating tokens**\n",
        "\n",
        "**Example:**\n",
        "\n",
        "**Tokens for `Wow! Tokens seems to be interesting. What do you think?` are**\n",
        "\n",
        "**`['Wow!', 'Tokens', 'seems', 'to', 'be', 'interesting.', 'What', 'do', 'you', 'think?']`**\n",
        "\n",
        "## Solution\n",
        "- **Using some library that can handle this issue.** ***Eg: NLTk***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78679cae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78679cae",
        "outputId": "8c263d98-b7fb-432e-bdec-0d28886d3584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tokens using split are:  ['Wow!', 'Tokens', 'seems', 'to', 'be', 'interesting.', 'What', 'do', 'you', 'think?']\n",
            "The tokens using NLTK are:  ['Wow', '!', 'Tokens', 'seems', 'to', 'be', 'interesting', '.', 'What', 'do', 'you', 'think', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Wow! Tokens seems to be interesting. What do you think?\"\n",
        "tokens_with_split = text.split()\n",
        "tokens_with_nltk = word_tokenize(text)     # word_tokenize is a NLTK function which has a inbuilt tokenizer\n",
        "\n",
        "print(\"The tokens using split are: \", tokens_with_split)\n",
        "print(\"The tokens using NLTK are: \", tokens_with_nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78be7a43",
      "metadata": {
        "id": "78be7a43"
      },
      "source": [
        "## We are in a process of making good Vocabulary. Punctuations are now being handled.\n",
        "\n",
        "**But wait, we are still missing something. What if we have a corpus with words `play, played, playing, plays` more repititive than other words?**\n",
        "\n",
        "**Our vocabulary will contains only the words that has similar meaning.**\n",
        "\n",
        "**What could be the solution?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65894945",
      "metadata": {
        "id": "65894945"
      },
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "- **Stemming and Lemmatization converts inflectional forms of each word into a common base or root.**\n",
        "\n",
        "**Example :**\n",
        "\n",
        "**`play, played, playing, plays` => `play`**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871004be",
      "metadata": {
        "id": "871004be"
      },
      "source": [
        "**Stemming: Cuts the end of the word, taking account of the common suffixes. It is best suited when context is not important.**\n",
        "\n",
        "**Example :**\n",
        "| Word    |Stem   |\n",
        "|---------|-------|\n",
        "| Studies | Studi |\n",
        "| Studying| Studi |\n",
        "\n",
        "**Lemmatization: takes into consideration the morphological analysis of the words. It is used in context analysis**\n",
        "\n",
        "**Example :**\n",
        "| Word    | Lemma  |\n",
        "|---------|-------|\n",
        "| likes | like |\n",
        "| like| like |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "196805d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "196805d9",
        "outputId": "beaa0d43-2ac1-4e06-984f-34ce755929ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The words before stemming:  ['Studies', 'Studying']\n",
            "The words after stemming:  ['studi', 'studi']\n",
            "The words before lemmatization:  ['likes', 'like']\n",
            "The words after lemmatization:  ['like', 'like']\n"
          ]
        }
      ],
      "source": [
        "stem_words = [\"Studies\", \"Studying\"]\n",
        "lemma_words = [\"likes\", \"like\"]\n",
        "\n",
        "# Defining Stemmer and Lemmatizer\n",
        "stemmer = PorterStemmer()    \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words_after_stemming = [stemmer.stem(word) for word in stem_words]\n",
        "words_after_lemmatizing = [lemmatizer.lemmatize(word) for word in lemma_words]\n",
        "\n",
        "print(\"The words before stemming: \", stem_words)\n",
        "print(\"The words after stemming: \", words_after_stemming)\n",
        "print(\"The words before lemmatization: \", lemma_words)\n",
        "print(\"The words after lemmatization: \", words_after_lemmatizing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c51ab0b",
      "metadata": {
        "id": "0c51ab0b"
      },
      "source": [
        "## Vectorization / Embedding\n",
        "\n",
        "- **Assigning numerical meaning to the texts.**\n",
        "\n",
        "- **Usually done in one of the following way:**\n",
        "    - **On Document/Corpus level**\n",
        "    - **On Token Level**\n",
        "    - **On Sub-Token Level**\n",
        "    \n",
        "***Quick Tip: The recent progress in NLP like ChatGPT is due to the improvement in the embedding and attention paid to the each embedding***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a6fa39",
      "metadata": {
        "id": "e9a6fa39"
      },
      "source": [
        "## Document Level Vectorization\n",
        "\n",
        "**Bag Of Words(BoW)**\n",
        "\n",
        "- **Bag of Words is a method for representing a piece of text/corpus as a collection of individual words, without considering the order in which they appear.**\n",
        "- **It is called \"Bag of Words\" because it treats the text as a \"bag\" of individual words, where the order of the words doesn't matter, just like the order of items in a bag doesn't matter.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aecf6e35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "aecf6e35",
        "outputId": "25ae2544-ed81-42e1-8cf6-a2b6896b7644"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Corpus</th>\n",
              "      <th>Spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hello</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Buy Crypto</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Meet me</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Send money</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi there</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Get Rich</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Corpus  Spam\n",
              "0       Hello     0\n",
              "1  Buy Crypto     1\n",
              "2     Meet me     0\n",
              "3  Send money     1\n",
              "4    Hi there     0\n",
              "5    Get Rich     1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating a sample dataset for spam/ham classification.\n",
        "corpus = [\"Hello\", \"Buy Crypto\", \"Meet me\", \"Send money\", \"Hi there\", \"Get Rich\"]\n",
        "labels = [0, 1, 0, 1, 0, 1] # 1: Spam, 0: Ham\n",
        "\n",
        "# Lets see in dataframe\n",
        "data = pd.DataFrame({\"Corpus\": corpus, \"Spam\": labels})\n",
        "data.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6cc83017",
      "metadata": {
        "id": "6cc83017"
      },
      "outputs": [],
      "source": [
        "# BoW using Sklearn\n",
        "\n",
        "# CountVectorizer is an sklearn class to create BoW\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "vector = vectorizer.transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d11d8d8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d11d8d8b",
        "outputId": "1dd983e1-3c47-4833-ed6c-3b987ae748ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The words in the bags(Vocabulary) are:  ['buy' 'crypto' 'get' 'hello' 'hi' 'me' 'meet' 'money' 'rich' 'send'\n",
            " 'there']\n",
            "\n",
            "Text: Hello\n",
            "Vector: [0 0 0 1 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "features = vectorizer.get_feature_names_out()\n",
        "print(\"The words in the bags(Vocabulary) are: \", features)\n",
        "\n",
        "# Sample document in BoW vector\n",
        "idx = 0\n",
        "text = corpus[idx]\n",
        "sample_vector = vector[idx]\n",
        "print(f\"\\nText: {text}\")\n",
        "print(f\"Vector: {sample_vector.toarray()[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "62c63219",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62c63219",
        "outputId": "700f4b17-5711-4c93-ee00-643413bfcbcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The text is:  I will send you money\n",
            "The vector is:  [0 0 0 0 0 0 0 1 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "# How vectorization is done for new text\n",
        "random_text = \"I will send you money\"\n",
        "random_vector = vectorizer.transform([random_text])\n",
        "\n",
        "print(\"The text is: \", random_text)\n",
        "print(\"The vector is: \", random_vector.toarray()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb78158",
      "metadata": {
        "id": "6bb78158"
      },
      "source": [
        "## Training a simple spam-ham classifier\n",
        "\n",
        "**We will use a simple Logistic Regression classifer. Because, the algorithm is very simple and we got very small dataset/corpus.**\n",
        "\n",
        "**The explanation of underlying math of Logistic Regression is outside of the scope for today's workshop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d0f5e9ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0f5e9ee",
        "outputId": "7bb9a599-adb7-4fe9-b0e9-10438203af1d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating a classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Defining input and output for training\n",
        "X = vector.toarray()\n",
        "y = labels\n",
        "\n",
        "# Training the classifier\n",
        "classifier.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cf525a98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf525a98",
        "outputId": "6cebd134-818b-405e-bde4-657f5c095baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The text is:  I will send you money\n",
            "Prediction:  1\n"
          ]
        }
      ],
      "source": [
        "# Testing the classifier in random text. For now, lets use the random text we defined in previous slides.\n",
        "prediction = classifier.predict(random_vector.toarray())\n",
        "print(\"The text is: \", random_text)\n",
        "print(\"Prediction: \", prediction[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "734b79ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "734b79ff",
        "outputId": "5a7d1f1f-4392-45d4-9676-4e16a2b7b1e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text:  Hi, I am Samip. Nice to meet you\n",
            "Prediction:  0\n"
          ]
        }
      ],
      "source": [
        "# Lets try for other text\n",
        "random_text = \"Hi, I am Samip. Nice to meet you\"\n",
        "\n",
        "prediction = classifier.predict(vectorizer.transform([random_text]).toarray())\n",
        "print(\"Text: \", random_text)\n",
        "print(\"Prediction: \", prediction[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6acbef0e",
      "metadata": {
        "id": "6acbef0e"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "  {\n",
        "    \"text\": \"I love this product, it works great!\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This movie was terrible, I would not recommend it to anyone.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The customer service was amazing, they were so helpful.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I am so disappointed with this restaurant, the food was cold and the service was slow.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I can't believe how good this book is, I couldn't put it down.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This hotel was a nightmare, there were bugs in the bed and the staff was rude.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I'm really happy with my new phone, it has all the features I wanted.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The traffic was so bad, it took me an hour to get to work.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I had an amazing time at the concert, the band was fantastic.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This product is terrible, it doesn't work at all.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The service at this restaurant was excellent, the staff was very attentive.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I was very disappointed with the hotel, the room was dirty and the staff was unhelpful.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I'm so happy with my new car, it drives like a dream.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This movie was fantastic, I would highly recommend it.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I had a terrible experience at the hair salon, the stylist didn't listen to me at all.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The food at this restaurant was delicious, I can't wait to go back.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I regret buying this product, it doesn't work as advertised.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The customer service at this store was terrible, the staff was rude and unhelpful.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I had a great time at the party, the music was fantastic and the food was delicious.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I'm so disappointed with this phone, it keeps freezing and the battery life is terrible.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The service at this hotel was excellent, the staff was very friendly and helpful.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I was very impressed with this product, it exceeded my expectations.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This book was a waste of money, I couldn't get past the first chapter.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This hotel was the worst, the room was dirty and the staff was unhelpful.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I had a great experience with customer service, they were so helpful and kind.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This movie was just okay, it wasn't great but it wasn't terrible either.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I was very impressed with this restaurant, the food was delicious and the service was excellent.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I was very disappointed with this product, it didn't work at all.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The staff at this store were very helpful, they went above and beyond to assist me.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This car is amazing, it has all the features I could ever want.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I had a terrible experience with this company, their customer service was terrible and their product didn't work.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The food at this restaurant was terrible, I would not recommend it to anyone.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I'm so happy with my new laptop, it's so fast and efficient.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This book was amazing, I couldn't put it down.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I had a terrible experience at this hotel, the room was dirty and the staff was rude.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The service at this restaurant was terrible, the staff was unhelpful and the food was cold.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I was very impressed with this service, they were so helpful and professional.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This movie was fantastic, I loved every minute of it.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I had a terrible experience at this store, the staff was rude and unhelpful.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The food at this restaurant was amazing, I can't wait to go back.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I was very disappointed with this product, it didn't work as advertised.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The customer service at this company was terrible, they were unresponsive and unhelpful.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I had a great time at the party, the atmosphere was amazing and the people were friendly.\",\n",
        "    \"label\": 1\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"This phone is terrible, it keeps freezing and the battery life is terrible.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"The service at this hotel was terrible, the staff was rude and unhelpful.\",\n",
        "    \"label\": 0\n",
        "  },\n",
        "  {\n",
        "    \"text\": \"I was very impressed with this product, it worked better than I expected.\",\n",
        "    \"label\": 1\n",
        "  }\n",
        "\n",
        "]\n",
        "\n",
        "# Creating a new dataset\n",
        "corpus_new = list()\n",
        "labels_new = list()\n",
        "\n",
        "for text_label in data:\n",
        "    text = text_label[\"text\"]\n",
        "    label = text_label[\"label\"]\n",
        "    corpus_new.append(text)\n",
        "    labels_new.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fc3ee5aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fc3ee5aa",
        "outputId": "0f193373-1af5-4a51-9818-bb9b02f452f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Corpus</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love this product, it works great!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This movie was terrible, I would not recommend...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The customer service was amazing, they were so...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I am so disappointed with this restaurant, the...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I can't believe how good this book is, I could...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Corpus  Sentiment\n",
              "0               I love this product, it works great!          1\n",
              "1  This movie was terrible, I would not recommend...          0\n",
              "2  The customer service was amazing, they were so...          1\n",
              "3  I am so disappointed with this restaurant, the...          0\n",
              "4  I can't believe how good this book is, I could...          1"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# New dataset for sentiment analysis\n",
        "\n",
        "df = pd.DataFrame({\"Corpus\": corpus_new, \"Sentiment\": labels_new})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e287305e",
      "metadata": {
        "id": "e287305e"
      },
      "outputs": [],
      "source": [
        "# Lets Preprocess the data\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)  # Tokenizing\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "processed_corpus = [preprocess(corpus) for corpus in corpus_new]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "34fffcec",
      "metadata": {
        "id": "34fffcec"
      },
      "outputs": [],
      "source": [
        "# Vectorizing the corpus\n",
        "\n",
        "vectorizer = CountVectorizer()  # Bag Of Words\n",
        "vectorizer.fit(processed_corpus)\n",
        "vectors = vectorizer.transform(processed_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6142f3e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6142f3e5",
        "outputId": "ed4cdd8b-9fa7-4d3b-ea51-b8717a257165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The words in the bags(Vocabulary) are:  ['above' 'advertised' 'all' 'am' 'amazing' 'an' 'and' 'anyone' 'assist'\n",
            " 'at' 'atmosphere' 'attentive' 'back' 'bad' 'band' 'battery' 'bed'\n",
            " 'believe' 'better' 'beyond' 'book' 'bug' 'but' 'buying' 'ca' 'car'\n",
            " 'chapter' 'cold' 'company' 'concert' 'could' 'customer' 'delicious' 'did'\n",
            " 'dirty' 'disappointed' 'doe' 'down' 'dream' 'drive' 'efficient' 'either'\n",
            " 'ever' 'every' 'exceeded' 'excellent' 'expectation' 'expected'\n",
            " 'experience' 'fantastic' 'fast' 'feature' 'first' 'food' 'freezing'\n",
            " 'friendly' 'get' 'go' 'good' 'great' 'ha' 'had' 'hair' 'happy' 'helpful'\n",
            " 'highly' 'hotel' 'hour' 'how' 'impressed' 'in' 'is' 'it' 'just' 'keep'\n",
            " 'kind' 'laptop' 'life' 'like' 'listen' 'love' 'loved' 'me' 'minute'\n",
            " 'money' 'movie' 'music' 'my' 'new' 'nightmare' 'not' 'of' 'okay' 'party'\n",
            " 'past' 'people' 'phone' 'product' 'professional' 'put' 'really'\n",
            " 'recommend' 'regret' 'restaurant' 'room' 'rude' 'salon' 'service' 'slow'\n",
            " 'so' 'staff' 'store' 'stylist' 'terrible' 'than' 'the' 'their' 'there'\n",
            " 'they' 'this' 'time' 'to' 'took' 'traffic' 'unhelpful' 'unresponsive'\n",
            " 'very' 'wa' 'wait' 'want' 'wanted' 'waste' 'went' 'were' 'with' 'work'\n",
            " 'worked' 'worst' 'would']\n",
            "\n",
            "Text: I love this product , it work great !\n",
            "Vector: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Looking on the vocabulary of new vectorizer\n",
        "\n",
        "features = vectorizer.get_feature_names_out()\n",
        "print(\"The words in the bags(Vocabulary) are: \", features)\n",
        "\n",
        "# Sample document in BoW vector\n",
        "idx = 0\n",
        "text = processed_corpus[idx]\n",
        "sample_vector = vectors[idx]\n",
        "print(f\"\\nText: {text}\")\n",
        "print(f\"Vector: {sample_vector.toarray()[0]}\")\n",
        "\n",
        "# We can control the size of vocabulary by changing max_features in CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2ab8f171",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ab8f171",
        "outputId": "17417a27-597a-405a-87bc-09351a524299"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Training of Sentiment Analysis model. This time again we are using Linear Regression for its simplicity.\n",
        "\n",
        "# Creating a classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Defining input and output for training\n",
        "X = vectors.toarray()\n",
        "y = labels_new\n",
        "\n",
        "# Training the classifier\n",
        "classifier.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b52fd312",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b52fd312",
        "outputId": "39f49b76-3379-4d23-edcc-ba8e17b9dfe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text:  I was very disappointed with this product, it didn't work as advertised.\n",
            "Prediction:  0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Lets try for other text\n",
        "# random_text = \"The movie is fantastic. I enjoyed every moment of it.\"\n",
        "random_text = \"I was very disappointed with this product, it didn't work as advertised.\"\n",
        "\n",
        "prediction = classifier.predict(vectorizer.transform([random_text]).toarray())\n",
        "print(\"Text: \", random_text)\n",
        "print(\"Prediction: \", prediction[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "HptJ5OJb5KDk",
      "metadata": {
        "id": "HptJ5OJb5KDk"
      },
      "source": [
        "### Day 5 contd ...\n",
        "\n",
        "Now lets save the classifier and vectorizers we built yesterday."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "76e34019",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Save the classifier and vectorizer as pickle files\n",
        "with open(\"data/classifier.pkl\", \"wb\") as f:\n",
        "    pickle.dump(classifier, f)\n",
        "\n",
        "with open(\"data/vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "itsnp_workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "ab52a52483d0eec32ff7d7080423e03ef28de93dffb19b666e2c0cb5ca580e20"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
